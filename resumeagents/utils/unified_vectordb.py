"""
í†µí•© ë²¡í„°DB - JSON í”„ë¡œí•„ì„ ë²¡í„°DBì— í†µí•©

ëª¨ë“  ì‚¬ìš©ì ì •ë³´(JSON í”„ë¡œí•„ + ê²½í—˜)ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°DBì— ì €ì¥í•˜ì—¬
ì—ì´ì „íŠ¸ë“¤ì´ ë‹¨ì¼ ì†ŒìŠ¤ì—ì„œ ëª¨ë“  ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
"""

import json
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
from sentence_transformers import SentenceTransformer
import faiss
import re
from collections import Counter
import math


class UnifiedVectorDB:
    """í†µí•© ë²¡í„°DB - JSON í”„ë¡œí•„ê³¼ ê²½í—˜ì„ ëª¨ë‘ ë²¡í„°í™”í•˜ì—¬ ì €ì¥"""
    
    def __init__(self, db_path: str = "db", model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        self.db_path = Path(db_path)
        self.db_path.mkdir(exist_ok=True)
        
        # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ë¡œë“œ
        self.encoder = SentenceTransformer(model_name)
        self.dimension = self.encoder.get_sentence_embedding_dimension()
        
        # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product (cosine similarity)
        self.data_entries = []  # ëª¨ë“  ë°ì´í„° ì—”íŠ¸ë¦¬ ì €ì¥
        self.metadata = []  # ë©”íƒ€ë°ì´í„° ì €ì¥
        
        # BM25ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ì¸ë±ìŠ¤
        self.keyword_index = {}  # ë‹¨ì–´ -> ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
        self.doc_frequencies = []  # ë¬¸ì„œë³„ ë‹¨ì–´ ë¹ˆë„
        self.doc_lengths = []  # ë¬¸ì„œë³„ ê¸¸ì´
        self.avg_doc_length = 0
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìºì‹œ
        self.query_cache = {}
        
        # ê¸°ì¡´ DB ë¡œë“œ
        self._load_existing_db()
    
    def add_profile_to_vectordb(self, profile_data: Dict[str, Any], profile_name: str) -> List[int]:
        """
        JSON í”„ë¡œí•„ì„ ë²¡í„°DBì— í†µí•© ì €ì¥
        
        Args:
            profile_data: JSON í”„ë¡œí•„ ë°ì´í„°
            profile_name: í”„ë¡œí•„ ì´ë¦„
        
        Returns:
            ì¶”ê°€ëœ ì—”íŠ¸ë¦¬ ID ë¦¬ìŠ¤íŠ¸
        """
        # ê¸°ì¡´ í”„ë¡œí•„ ì—”íŠ¸ë¦¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        self._remove_profile_entries(profile_name)
        
        entry_ids = []
        
        # 1. ê°œì¸ì •ë³´ ì¶”ê°€
        personal_info = profile_data.get("personal_info", {})
        if personal_info:
            personal_text = self._personal_info_to_text(personal_info)
            entry_id = self._add_entry(personal_text, {
                "type": "personal_info",
                "profile_name": profile_name,
                "data": personal_info,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 2. í•™ë ¥ ì •ë³´ ì¶”ê°€
        for i, education in enumerate(profile_data.get("education", [])):
            education_text = self._education_to_text(education)
            entry_id = self._add_entry(education_text, {
                "type": "education",
                "profile_name": profile_name,
                "data": education,
                "index": i,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 3. ê²½ë ¥ ì •ë³´ ì¶”ê°€
        for i, experience in enumerate(profile_data.get("work_experience", [])):
            experience_text = self._work_experience_to_text(experience)
            entry_id = self._add_entry(experience_text, {
                "type": "work_experience",
                "profile_name": profile_name,
                "data": experience,
                "index": i,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 4. í”„ë¡œì íŠ¸ ì •ë³´ ì¶”ê°€
        for i, project in enumerate(profile_data.get("projects", [])):
            project_text = self._project_to_text(project)
            entry_id = self._add_entry(project_text, {
                "type": "project",
                "profile_name": profile_name,
                "data": project,
                "index": i,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 5. ê¸°ìˆ  ìŠ¤íƒ ì¶”ê°€
        skills = profile_data.get("skills", {})
        if skills:
            skills_text = self._skills_to_text(skills)
            entry_id = self._add_entry(skills_text, {
                "type": "skills",
                "profile_name": profile_name,
                "data": skills,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 6. ìê²©ì¦ ì¶”ê°€
        for i, certification in enumerate(profile_data.get("certifications", [])):
            cert_text = self._certification_to_text(certification)
            entry_id = self._add_entry(cert_text, {
                "type": "certification",
                "profile_name": profile_name,
                "data": certification,
                "index": i,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 7. ìˆ˜ìƒë‚´ì—­ ì¶”ê°€
        for i, award in enumerate(profile_data.get("awards", [])):
            award_text = self._award_to_text(award)
            entry_id = self._add_entry(award_text, {
                "type": "award",
                "profile_name": profile_name,
                "data": award,
                "index": i,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 8. ì»¤ë¦¬ì–´ ëª©í‘œ ì¶”ê°€
        career_goals = profile_data.get("career_goals", {})
        if career_goals:
            goals_text = self._career_goals_to_text(career_goals)
            entry_id = self._add_entry(goals_text, {
                "type": "career_goals",
                "profile_name": profile_name,
                "data": career_goals,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # 9. ê´€ì‹¬ì‚¬ ì¶”ê°€
        interests = profile_data.get("interests", [])
        if interests:
            interests_text = self._interests_to_text(interests)
            entry_id = self._add_entry(interests_text, {
                "type": "interests",
                "profile_name": profile_name,
                "data": interests,
                "timestamp": datetime.now().isoformat()
            })
            entry_ids.append(entry_id)
        
        # ë²¡í„°DB ì €ì¥ (ì¤‘ìš”!)
        self.save_db()
        
        print(f"âœ… í”„ë¡œí•„ '{profile_name}' í†µí•© ë²¡í„°DBì— ì¶”ê°€ ì™„ë£Œ: {len(entry_ids)}ê°œ ì—”íŠ¸ë¦¬")
        return entry_ids
    
    def search_unified_profile(self, query: str, profile_name: str = None, data_types: List[str] = None, 
                             top_k: int = 5, min_score: float = 0.1, search_mode: str = "hybrid") -> List[Dict[str, Any]]:
        """
        í†µí•© ë²¡í„°DBì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            profile_name: íŠ¹ì • í”„ë¡œí•„ë¡œ ì œí•œ (ì„ íƒì‚¬í•­)
            data_types: ê²€ìƒ‰í•  ë°ì´í„° ìœ í˜• ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            min_score: ìµœì†Œ ê´€ë ¨ë„ ì ìˆ˜ (ê¸°ë³¸ê°’: 0.1)
            search_mode: "semantic", "keyword", "hybrid" (ê¸°ë³¸ê°’: hybrid)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ê´€ë ¨ë„ ìˆœ ì •ë ¬)
        """
        if self.index.ntotal == 0:
            return []
        
        # ì¿¼ë¦¬ í™•ì¥ (ë™ì˜ì–´, ê´€ë ¨ì–´ ì¶”ê°€)
        expanded_query = self._expand_query(query)
        
        if search_mode == "hybrid":
            return self._hybrid_search(expanded_query, profile_name, data_types, top_k, min_score)
        elif search_mode == "semantic":
            return self._semantic_search(expanded_query, profile_name, data_types, top_k, min_score)
        elif search_mode == "keyword":
            return self._keyword_search(expanded_query, profile_name, data_types, top_k, min_score)
        else:
            return self._hybrid_search(expanded_query, profile_name, data_types, top_k, min_score)
    
    def _expand_query(self, query: str) -> str:
        """ì¿¼ë¦¬ í™•ì¥ - ë°ì´í„°/AI íŠ¹í™” ë™ì˜ì–´, ê´€ë ¨ì–´, ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í™•ì¥"""
        # ê¸°ìˆ  ë¶„ì•¼ ë™ì˜ì–´ ì‚¬ì „ (ë°ì´í„°/AI íŠ¹í™” í™•ì¥)
        synonyms = {
            # ê¸°ì¡´ ê¸°ìˆ  ë™ì˜ì–´
            "ë°±ì—”ë“œ": ["backend", "ì„œë²„", "server", "API", "ì„œë²„ì‚¬ì´ë“œ"],
            "í”„ë¡ íŠ¸ì—”ë“œ": ["frontend", "í´ë¼ì´ì–¸íŠ¸", "client", "UI", "UX", "ì›¹"],
            "íŒŒì´ì¬": ["python", "django", "flask", "fastapi", "py"],
            "ìë°”ìŠ¤í¬ë¦½íŠ¸": ["javascript", "js", "node", "react", "vue", "angular"],
            "ë°ì´í„°ë² ì´ìŠ¤": ["database", "db", "mysql", "postgresql", "mongodb", "sql"],
            "í´ë¼ìš°ë“œ": ["cloud", "aws", "azure", "gcp", "kubernetes", "docker"],
            "ê°œë°œ": ["development", "coding", "programming", "êµ¬í˜„", "ì½”ë”©"],
            "í”„ë¡œì íŠ¸": ["project", "ì‘ì—…", "ì—…ë¬´", "ê°œë°œ", "ì‹œìŠ¤í…œ"],
            "ê²½í—˜": ["experience", "ì´ë ¥", "ì—…ë¬´", "í”„ë¡œì íŠ¸", "ì°¸ì—¬"],
            "ì„±ëŠ¥": ["performance", "ìµœì í™”", "optimization", "ì†ë„", "íŠœë‹"],
            "ë³´ì•ˆ": ["security", "ì•”í˜¸í™”", "ì¸ì¦", "authorization", "ê¶Œí•œ"],
            "ìµœì í™”": ["optimization", "performance", "tuning", "ê°œì„ ", "í–¥ìƒ"],
            "ì•„í‚¤í…ì²˜": ["architecture", "ì„¤ê³„", "design", "êµ¬ì¡°", "ì‹œìŠ¤í…œ"],
            
            # === ë°ì´í„°/AI íŠ¹í™” ë™ì˜ì–´ ===
            # ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§
            "ë°ì´í„°ì—”ì§€ë‹ˆì–´": ["data engineer", "ë°ì´í„°ì—”ì§€ë‹ˆì–´ë§", "data engineering", "ETL", "íŒŒì´í”„ë¼ì¸"],
            "ë°ì´í„°íŒŒì´í”„ë¼ì¸": ["data pipeline", "ETL", "ELT", "ë°ì´í„°í”Œë¡œìš°", "workflow", "airflow"],
            "ETL": ["extract transform load", "ë°ì´í„°íŒŒì´í”„ë¼ì¸", "ë°ì´í„°ì²˜ë¦¬", "ë°°ì¹˜ì²˜ë¦¬"],
            "ELT": ["extract load transform", "ë°ì´í„°ë ˆì´í¬", "í´ë¼ìš°ë“œë°ì´í„°"],
            "ìŠ¤íŠ¸ë¦¬ë°": ["streaming", "ì‹¤ì‹œê°„", "real-time", "kafka", "kinesis", "spark streaming"],
            "ë°°ì¹˜ì²˜ë¦¬": ["batch processing", "ìŠ¤ì¼€ì¤„ë§", "cron", "airflow", "luigi"],
            
            # ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤
            "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤": ["data science", "ë°ì´í„°ë¶„ì„", "í†µê³„ë¶„ì„", "ì˜ˆì¸¡ëª¨ë¸ë§"],
            "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸": ["data scientist", "ë¶„ì„ê°€", "analyst", "ì—°êµ¬ì›"],
            "ë°ì´í„°ë¶„ì„": ["data analysis", "analytics", "í†µê³„", "statistics", "ì‹œê°í™”"],
            "í†µê³„": ["statistics", "í†µê³„í•™", "í™•ë¥ ", "probability", "ì¶”ë¡ "],
            "ì˜ˆì¸¡ëª¨ë¸ë§": ["predictive modeling", "forecasting", "ì˜ˆì¸¡", "ëª¨ë¸ë§", "regression"],
            "ë¶„ë¥˜": ["classification", "classifier", "supervised learning", "ì§€ë„í•™ìŠµ"],
            "íšŒê·€": ["regression", "linear regression", "ì˜ˆì¸¡", "ì—°ì†ê°’"],
            "í´ëŸ¬ìŠ¤í„°ë§": ["clustering", "êµ°ì§‘í™”", "unsupervised", "ë¹„ì§€ë„í•™ìŠµ"],
            
            # ë¨¸ì‹ ëŸ¬ë‹/AI
            "ë¨¸ì‹ ëŸ¬ë‹": ["machine learning", "ML", "AI", "ì¸ê³µì§€ëŠ¥", "ë”¥ëŸ¬ë‹", "ëª¨ë¸", "í•™ìŠµ"],
            "ë”¥ëŸ¬ë‹": ["deep learning", "neural network", "ì‹ ê²½ë§", "CNN", "RNN", "transformer"],
            "ì¸ê³µì§€ëŠ¥": ["artificial intelligence", "AI", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìë™í™”"],
            "ì‹ ê²½ë§": ["neural network", "ë”¥ëŸ¬ë‹", "í¼ì…‰íŠ¸ë¡ ", "ë ˆì´ì–´", "ë…¸ë“œ"],
            "ìì—°ì–´ì²˜ë¦¬": ["NLP", "natural language processing", "í…ìŠ¤íŠ¸ë¶„ì„", "ì–¸ì–´ëª¨ë¸"],
            "ì»´í“¨í„°ë¹„ì „": ["computer vision", "CV", "ì´ë¯¸ì§€ì²˜ë¦¬", "ê°ì²´ì¸ì‹", "CNN"],
            "ì¶”ì²œì‹œìŠ¤í…œ": ["recommendation system", "collaborative filtering", "ê°œì¸í™”", "ì¶”ì²œì—”ì§„"],
            "ê°•í™”í•™ìŠµ": ["reinforcement learning", "RL", "ì—ì´ì „íŠ¸", "ë³´ìƒ", "ì •ì±…"],
            
            # ë¹…ë°ì´í„° ê¸°ìˆ 
            "ë¹…ë°ì´í„°": ["big data", "ëŒ€ìš©ëŸ‰ë°ì´í„°", "ë¶„ì‚°ì²˜ë¦¬", "hadoop", "spark"],
            "í•˜ë‘¡": ["hadoop", "HDFS", "mapreduce", "ë¶„ì‚°ì €ì¥", "í´ëŸ¬ìŠ¤í„°"],
            "ìŠ¤íŒŒí¬": ["spark", "apache spark", "ë¶„ì‚°ì²˜ë¦¬", "ì¸ë©”ëª¨ë¦¬", "ì‹¤ì‹œê°„"],
            "ì¹´í”„ì¹´": ["kafka", "ë©”ì‹œì§•", "ìŠ¤íŠ¸ë¦¬ë°", "ì´ë²¤íŠ¸", "í", "ì‹¤ì‹œê°„"],
            "ì—˜ë¼ìŠ¤í‹±ì„œì¹˜": ["elasticsearch", "ê²€ìƒ‰ì—”ì§„", "ë¡œê·¸ë¶„ì„", "ì¸ë±ì‹±", "kibana"],
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŠ¹í™”
            "NoSQL": ["nosql", "mongodb", "cassandra", "redis", "ë¹„ê´€ê³„í˜•"],
            "ë°ì´í„°ì›¨ì–´í•˜ìš°ìŠ¤": ["data warehouse", "DW", "OLAP", "dimensional modeling"],
            "ë°ì´í„°ë ˆì´í¬": ["data lake", "S3", "ì €ì¥ì†Œ", "ì›ì‹œë°ì´í„°", "ìŠ¤í‚¤ë§ˆì˜¨ë¦¬ë“œ"],
            "ë°ì´í„°ë§ˆíŠ¸": ["data mart", "ë¶€ì„œë³„ë°ì´í„°", "ìš”ì•½ë°ì´í„°", "OLAP"],
            
            # í´ë¼ìš°ë“œ/MLOps
            "MLOps": ["mlops", "ëª¨ë¸ìš´ì˜", "CI/CD", "ëª¨ë¸ë°°í¬", "ëª¨ë¸ê´€ë¦¬"],
            "ëª¨ë¸ë°°í¬": ["model deployment", "serving", "ì¶”ë¡ ", "production", "API"],
            "ëª¨ë¸ëª¨ë‹ˆí„°ë§": ["model monitoring", "drift detection", "ì„±ëŠ¥ì¶”ì ", "A/Bí…ŒìŠ¤íŠ¸"],
            "í”¼ì²˜ì—”ì§€ë‹ˆì–´ë§": ["feature engineering", "ë³€ìˆ˜ìƒì„±", "ì „ì²˜ë¦¬", "í”¼ì²˜ì„ íƒ"],
            "í•˜ì´í¼íŒŒë¼ë¯¸í„°": ["hyperparameter", "íŠœë‹", "ìµœì í™”", "ê·¸ë¦¬ë“œì„œì¹˜"],
            
            # ì‹œê°í™”/BI
            "ì‹œê°í™”": ["visualization", "ì°¨íŠ¸", "ê·¸ë˜í”„", "ëŒ€ì‹œë³´ë“œ", "plotting"],
            "ëŒ€ì‹œë³´ë“œ": ["dashboard", "BI", "business intelligence", "ë¦¬í¬íŒ…"],
            "BI": ["business intelligence", "ëŒ€ì‹œë³´ë“œ", "ë¦¬í¬íŒ…", "ë¶„ì„ë„êµ¬"],
            "íƒœë¸”ë¡œ": ["tableau", "ì‹œê°í™”ë„êµ¬", "ëŒ€ì‹œë³´ë“œ", "ì…€í”„ì„œë¹„ìŠ¤"],
            
            # í”„ë¡œê·¸ë˜ë°/ë„êµ¬
            "R": ["Rì–¸ì–´", "í†µê³„ë¶„ì„", "ë°ì´í„°ë¶„ì„", "ggplot", "dplyr"],
            "SQL": ["ë°ì´í„°ë² ì´ìŠ¤", "ì¿¼ë¦¬", "ì¡°ì¸", "ì§‘ê³„", "ë¶„ì„"],
            "ì£¼í”¼í„°": ["jupyter", "notebook", "ipython", "ë¶„ì„í™˜ê²½", "í”„ë¡œí† íƒ€ì´í•‘"],
            "ë„ì»¤": ["docker", "ì»¨í…Œì´ë„ˆ", "ê°€ìƒí™”", "ë°°í¬", "í™˜ê²½ê´€ë¦¬"],
            "git": ["ë²„ì „ê´€ë¦¬", "í˜‘ì—…", "github", "gitlab", "ì†ŒìŠ¤ê´€ë¦¬"],
            
            # ë„ë©”ì¸ íŠ¹í™”
            "A/Bí…ŒìŠ¤íŠ¸": ["AB test", "ì‹¤í—˜ì„¤ê³„", "í†µê³„ê²€ì •", "ê°€ì„¤ê²€ì¦"],
            "ì¶”ì²œì—”ì§„": ["recommendation engine", "í˜‘ì—…í•„í„°ë§", "ê°œì¸í™”", "ì¶”ì²œì‹œìŠ¤í…œ"],
            "ì´ìƒíƒì§€": ["anomaly detection", "outlier", "fraud detection", "ë¹„ì •ìƒ"],
            "ì‹œê³„ì—´": ["time series", "ì‹œê°„ë°ì´í„°", "ì˜ˆì¸¡", "íŠ¸ë Œë“œ", "ê³„ì ˆì„±"],
            "í…ìŠ¤íŠ¸ë§ˆì´ë‹": ["text mining", "ìì—°ì–´ì²˜ë¦¬", "ê°ì •ë¶„ì„", "í† í”½ëª¨ë¸ë§"]
        }
        
        # ë°ì´í„°/AI ê¸°ìˆ  ìŠ¤íƒ ê´€ë ¨ì–´ (ëŒ€í­ í™•ì¥)
        tech_relations = {
            # Python ìƒíƒœê³„
            "python": ["pandas", "numpy", "scikit-learn", "ë°ì´í„°ë¶„ì„", "ë¨¸ì‹ ëŸ¬ë‹"],
            "pandas": ["ë°ì´í„°í”„ë ˆì„", "ì „ì²˜ë¦¬", "ë°ì´í„°ì¡°ì‘", "ë¶„ì„"],
            "numpy": ["ìˆ˜ì¹˜ê³„ì‚°", "ë°°ì—´", "ì„ í˜•ëŒ€ìˆ˜", "ê³¼í•™ê³„ì‚°"],
            "scikit-learn": ["ë¨¸ì‹ ëŸ¬ë‹", "ë¶„ë¥˜", "íšŒê·€", "í´ëŸ¬ìŠ¤í„°ë§"],
            "matplotlib": ["ì‹œê°í™”", "í”Œë¡¯", "ì°¨íŠ¸", "ê·¸ë˜í”„"],
            "seaborn": ["í†µê³„ì‹œê°í™”", "íˆíŠ¸ë§µ", "ë¶„í¬", "ìƒê´€ê´€ê³„"],
            
            # ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
            "tensorflow": ["ë”¥ëŸ¬ë‹", "ì‹ ê²½ë§", "ëª¨ë¸í›ˆë ¨", "ì¼€ë¼ìŠ¤"],
            "pytorch": ["ë”¥ëŸ¬ë‹", "ë™ì ê·¸ë˜í”„", "ì—°êµ¬", "ì‹¤í—˜"],
            "keras": ["ë”¥ëŸ¬ë‹", "ê³ ìˆ˜ì¤€API", "ë¹ ë¥¸í”„ë¡œí† íƒ€ì´í•‘"],
            
            # ë¹…ë°ì´í„° ìƒíƒœê³„
            "spark": ["ë¶„ì‚°ì²˜ë¦¬", "ë¹…ë°ì´í„°", "ì¸ë©”ëª¨ë¦¬", "ìŠ¤ì¼€ì¼ë§"],
            "hadoop": ["ë¶„ì‚°ì €ì¥", "HDFS", "ë§µë¦¬ë“€ìŠ¤", "í´ëŸ¬ìŠ¤í„°"],
            "kafka": ["ì‹¤ì‹œê°„ìŠ¤íŠ¸ë¦¬ë°", "ë©”ì‹œì§€í", "ì´ë²¤íŠ¸ì²˜ë¦¬"],
            "airflow": ["ì›Œí¬í”Œë¡œìš°", "ìŠ¤ì¼€ì¤„ë§", "ë°ì´í„°íŒŒì´í”„ë¼ì¸", "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜"],
            
            # ë°ì´í„°ë² ì´ìŠ¤
            "postgresql": ["ê´€ê³„í˜•DB", "ACID", "ë³µì¡ì¿¼ë¦¬", "ë¶„ì„"],
            "mongodb": ["NoSQL", "ë¬¸ì„œDB", "ìŠ¤í‚¤ë§ˆë¦¬ìŠ¤", "í™•ì¥ì„±"],
            "redis": ["ì¸ë©”ëª¨ë¦¬", "ìºì‹œ", "ì„¸ì…˜ì €ì¥", "ì‹¤ì‹œê°„"],
            "elasticsearch": ["ê²€ìƒ‰ì—”ì§„", "ì „ë¬¸ê²€ìƒ‰", "ë¡œê·¸ë¶„ì„", "ì§‘ê³„"],
            
            # í´ë¼ìš°ë“œ í”Œë«í¼
            "aws": ["S3", "EMR", "Redshift", "SageMaker", "Lambda"],
            "gcp": ["BigQuery", "Dataflow", "AI Platform", "Cloud ML"],
            "azure": ["Synapse", "Data Factory", "Machine Learning", "Cognitive Services"],
            
            # BI/ì‹œê°í™” ë„êµ¬
            "tableau": ["ëŒ€ì‹œë³´ë“œ", "ì…€í”„ì„œë¹„ìŠ¤BI", "ë“œë˜ê·¸ì•¤ë“œë¡­", "ì‹œê°í™”"],
            "powerbi": ["ë§ˆì´í¬ë¡œì†Œí”„íŠ¸", "ë¹„ì¦ˆë‹ˆìŠ¤ì¸í…”ë¦¬ì „ìŠ¤", "ë¦¬í¬íŒ…"],
            "looker": ["ëª¨ë˜BI", "ë°ì´í„°ëª¨ë¸ë§", "SQLê¸°ë°˜"],
            
            # MLOps ë„êµ¬
            "mlflow": ["ëª¨ë¸ë¼ì´í”„ì‚¬ì´í´", "ì‹¤í—˜ì¶”ì ", "ëª¨ë¸ë ˆì§€ìŠ¤íŠ¸ë¦¬"],
            "kubeflow": ["ì¿ ë²„ë„¤í‹°ìŠ¤", "MLì›Œí¬í”Œë¡œìš°", "íŒŒì´í”„ë¼ì¸"],
            "dvc": ["ë°ì´í„°ë²„ì „ê´€ë¦¬", "MLì‹¤í—˜", "ì¬í˜„ê°€ëŠ¥ì„±"],
            
            # íŠ¹í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
            "lightgbm": ["ê·¸ë˜ë””ì–¸íŠ¸ë¶€ìŠ¤íŒ…", "ë¹ ë¥¸í•™ìŠµ", "ë©”ëª¨ë¦¬íš¨ìœ¨"],
            "xgboost": ["ì•™ìƒë¸”", "ë¶€ìŠ¤íŒ…", "êµ¬ì¡°í™”ë°ì´í„°", "ê²½ì§„ëŒ€íšŒ"],
            "catboost": ["ë²”ì£¼í˜•ë°ì´í„°", "ê·¸ë˜ë””ì–¸íŠ¸ë¶€ìŠ¤íŒ…", "ìë™í™”"],
            "spacy": ["ìì—°ì–´ì²˜ë¦¬", "NER", "í’ˆì‚¬íƒœê¹…", "ì–¸ì–´ëª¨ë¸"],
            "nltk": ["ìì—°ì–´ì²˜ë¦¬", "í† í°í™”", "í˜•íƒœì†Œë¶„ì„", "ì½”í¼ìŠ¤"],
            "opencv": ["ì»´í“¨í„°ë¹„ì „", "ì´ë¯¸ì§€ì²˜ë¦¬", "ê°ì²´ì¸ì‹", "ì˜ìƒë¶„ì„"],
            
            # í†µê³„/ìˆ˜í•™ ë„êµ¬
            "scipy": ["ê³¼í•™ê³„ì‚°", "ìµœì í™”", "í†µê³„", "ì‹ í˜¸ì²˜ë¦¬"],
            "statsmodels": ["í†µê³„ëª¨ë¸ë§", "íšŒê·€ë¶„ì„", "ì‹œê³„ì—´", "ê°€ì„¤ê²€ì •"],
            "networkx": ["ê·¸ë˜í”„ë¶„ì„", "ë„¤íŠ¸ì›Œí¬", "ì†Œì…œë„¤íŠ¸ì›Œí¬", "ê´€ê³„ë¶„ì„"]
        }
        
        expanded_terms = [query]
        query_lower = query.lower()
        
        # ê¸°ë³¸ ë™ì˜ì–´ í™•ì¥
        for key, values in synonyms.items():
            if key in query_lower:
                expanded_terms.extend(values[:4])  # ë°ì´í„° ë¶„ì•¼ëŠ” ë” ë§ì€ ë™ì˜ì–´ ì‚¬ìš©
            for value in values:
                if value in query_lower:
                    expanded_terms.append(key)
                    expanded_terms.extend([v for v in values if v != value][:3])
        
        # ê¸°ìˆ  ìŠ¤íƒ ê´€ë ¨ì–´ í™•ì¥
        for tech, relations in tech_relations.items():
            if tech in query_lower:
                expanded_terms.extend(relations[:3])  # ê´€ë ¨ ê¸°ìˆ ë„ ë” ë§ì´ í¬í•¨
        
        # ë°ì´í„°/AI íŠ¹í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ë¡ 
        data_contexts = {
            "ë¶„ì„": ["ë°ì´í„°ë¶„ì„", "í†µê³„", "ì¸ì‚¬ì´íŠ¸", "ë¦¬í¬íŒ…"],
            "ëª¨ë¸": ["ë¨¸ì‹ ëŸ¬ë‹", "ì˜ˆì¸¡", "ì•Œê³ ë¦¬ì¦˜", "í›ˆë ¨"],
            "ì²˜ë¦¬": ["ì „ì²˜ë¦¬", "ETL", "íŒŒì´í”„ë¼ì¸", "ë³€í™˜"],
            "ì‹œê°í™”": ["ì°¨íŠ¸", "ëŒ€ì‹œë³´ë“œ", "ê·¸ë˜í”„", "í”Œë¡¯"],
            "ì˜ˆì¸¡": ["ëª¨ë¸ë§", "í¬ìºìŠ¤íŒ…", "íšŒê·€", "ë¶„ë¥˜"],
            "ì¶”ì²œ": ["ê°œì¸í™”", "í˜‘ì—…í•„í„°ë§", "ë­í‚¹", "ë§¤ì¹­"]
        }
        
        for context, related_terms in data_contexts.items():
            if context in query_lower:
                expanded_terms.extend(related_terms[:2])
        
        # ì¤‘ë³µ ì œê±° ë° ê°€ì¤‘ì¹˜ ì ìš©
        unique_terms = list(dict.fromkeys(expanded_terms))  # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
        
        # ë°ì´í„°/AI ë¶„ì•¼ëŠ” ì›ë³¸ ì¿¼ë¦¬ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ (5ë°° ë°˜ë³µ)
        weighted_query = f"{query} {query} {query} {query} {query} " + " ".join(unique_terms[1:])
        
        return weighted_query
    
    def _hybrid_search(self, query: str, profile_name: str, data_types: List[str], 
                      top_k: int, min_score: float) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ)"""
        # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼
        semantic_results = self._semantic_search(query, profile_name, data_types, top_k * 2, min_score * 0.7)
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        keyword_results = self._keyword_search(query, profile_name, data_types, top_k * 2, min_score * 0.5)
        
        # ê²°ê³¼ ë³‘í•© ë° ì ìˆ˜ ì¡°í•©
        combined_results = {}
        
        # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ê°€ì¤‘ì¹˜ 0.7)
        for result in semantic_results:
            key = self._get_result_key(result)
            combined_results[key] = {
                **result,
                "semantic_score": result["score"],
                "keyword_score": 0.0,
                "combined_score": result["score"] * 0.7
            }
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€/ì—…ë°ì´íŠ¸ (ê°€ì¤‘ì¹˜ 0.3)
        for result in keyword_results:
            key = self._get_result_key(result)
            if key in combined_results:
                # ê¸°ì¡´ ê²°ê³¼ ì—…ë°ì´íŠ¸
                combined_results[key]["keyword_score"] = result["score"]
                combined_results[key]["combined_score"] = (
                    combined_results[key]["semantic_score"] * 0.7 + 
                    result["score"] * 0.3
                )
            else:
                # ìƒˆë¡œìš´ ê²°ê³¼ ì¶”ê°€
                combined_results[key] = {
                    **result,
                    "semantic_score": 0.0,
                    "keyword_score": result["score"],
                    "combined_score": result["score"] * 0.3
                }
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        final_results = list(combined_results.values())
        final_results.sort(key=lambda x: x["combined_score"], reverse=True)
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì—…ë°ì´íŠ¸
        for result in final_results:
            result["score"] = result["combined_score"]
            result["search_method"] = "hybrid_semantic_keyword"
        
        return final_results[:top_k]
    
    def _get_result_key(self, result: Dict[str, Any]) -> str:
        """ê²°ê³¼ ê³ ìœ  í‚¤ ìƒì„±"""
        metadata = result["metadata"]
        return f"{metadata.get('profile_name')}_{metadata.get('type')}_{metadata.get('timestamp')}"
    
    def get_profile_summary(self, profile_name: str) -> Dict[str, Any]:
        """íŠ¹ì • í”„ë¡œí•„ì˜ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        profile_entries = [entry for entry in self.metadata if entry.get("profile_name") == profile_name]
        
        if not profile_entries:
            return {"error": f"í”„ë¡œí•„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {profile_name}"}
        
        # ìœ í˜•ë³„ í†µê³„
        type_counts = {}
        for entry in profile_entries:
            entry_type = entry.get("type", "unknown")
            type_counts[entry_type] = type_counts.get(entry_type, 0) + 1
        
        # ê°œì¸ì •ë³´ ì¶”ì¶œ
        personal_info = None
        for entry in profile_entries:
            if entry.get("type") == "personal_info":
                personal_info = entry.get("data", {})
                break
        
        return {
            "profile_name": profile_name,
            "personal_info": personal_info or {},
            "type_counts": type_counts,
            "total_entries": len(profile_entries),
            "last_updated": max(entry.get("timestamp", "") for entry in profile_entries)
        }
    
    def get_agent_context(self, profile_name: str, agent_type: str, task_context: str = None) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ë³„ ë§ì¶¤í˜• ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            profile_name: í”„ë¡œí•„ ì´ë¦„
            agent_type: ì—ì´ì „íŠ¸ ìœ í˜•
            task_context: ì‘ì—… ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            ì—ì´ì „íŠ¸ë³„ ì»¨í…ìŠ¤íŠ¸
        """
        # ì—ì´ì „íŠ¸ë³„ ê²€ìƒ‰ ì „ëµ
        agent_strategies = {
            "company_analyst": {
                "query": "íšŒì‚¬ ë¶„ì„ì— í•„ìš”í•œ ê²½í—˜ê³¼ ëª©í‘œ",
                "data_types": ["work_experience", "career_goals", "personal_info"],
                "top_k": 5
            },
            "jd_analyst": {
                "query": "ì§ë¬´ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ê¸°ìˆ ê³¼ ê²½í—˜",
                "data_types": ["work_experience", "skills", "projects"],
                "top_k": 5
            },
            "question_guide": {
                "query": task_context or "ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ì— ê´€ë ¨ëœ ê²½í—˜",
                "data_types": ["work_experience", "projects", "education"],
                "top_k": 5
            },
            "experience_guide": {
                "query": "STAR ë°©ë²•ë¡ ì— ì í•©í•œ êµ¬ì²´ì  ê²½í—˜",
                "data_types": ["work_experience", "projects"],
                "top_k": 3
            },
            "writing_guide": {
                "query": "ê¸€ì“°ê¸° ì „ëµì— í•„ìš”í•œ ê²½í—˜ê³¼ ëª©í‘œ",
                "data_types": ["work_experience", "projects", "career_goals"],
                "top_k": 5
            }
        }
        
        strategy = agent_strategies.get(agent_type, {
            "query": "ê´€ë ¨ ê²½í—˜",
            "data_types": ["work_experience", "projects"],
            "top_k": 3
        })
        
        # í†µí•© ë²¡í„°DBì—ì„œ ê²€ìƒ‰
        relevant_entries = self.search_unified_profile(
            query=strategy["query"],
            profile_name=profile_name,
            data_types=strategy["data_types"],
            top_k=strategy["top_k"]
        )
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = {
            "profile_name": profile_name,
            "agent_type": agent_type,
            "task_context": task_context,
            "relevant_entries": relevant_entries,
            "strategy": strategy,
            "vectordb_enabled": True,
            "context_timestamp": datetime.now().isoformat()
        }
        
        return context
    
    def _add_entry(self, text: str, metadata: Dict[str, Any]) -> int:
        """ë²¡í„°DBì— ì—”íŠ¸ë¦¬ ì¶”ê°€ (ë©”ëª¨ë¦¬ ìµœì í™” + í‚¤ì›Œë“œ ì¸ë±ìŠ¤)"""
        # í…ìŠ¤íŠ¸ ë²¡í„°í™”
        embedding = self.encoder.encode([text])
        faiss.normalize_L2(embedding)
        
        # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(embedding)
        
        # ë©”íƒ€ë°ì´í„° ìµœì í™” (í° ë°ì´í„°ëŠ” IDë§Œ ì €ì¥)
        optimized_metadata = {
            "type": metadata.get("type"),
            "profile_name": metadata.get("profile_name"),
            "timestamp": metadata.get("timestamp"),
            "index": metadata.get("index")  # ë°°ì—´ ì¸ë±ìŠ¤ë§Œ ì €ì¥
        }
        
        # ë°ì´í„°ëŠ” ë³„ë„ ì €ì¥ (í•„ìš”ì‹œì—ë§Œ ë¡œë“œ)
        entry_id = len(self.data_entries)
        self.data_entries.append(text)
        self.metadata.append(optimized_metadata)
        
        # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self._update_keyword_index(text, entry_id)
        
        # ì›ë³¸ ë°ì´í„°ëŠ” ë³„ë„ íŒŒì¼ì— ì €ì¥
        self._save_entry_data(entry_id, metadata.get("data", {}))
        
        return entry_id
    
    def _update_keyword_index(self, text: str, doc_id: int):
        """ìƒˆ ë¬¸ì„œì— ëŒ€í•´ í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        tokens = self._tokenize(text)
        doc_freq = Counter(tokens)
        
        # ë¬¸ì„œ ë¹ˆë„ì™€ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸ í™•ì¥ (í•„ìš”í•œ ê²½ìš°)
        while len(self.doc_frequencies) <= doc_id:
            self.doc_frequencies.append({})
            self.doc_lengths.append(0)
        
        self.doc_frequencies[doc_id] = doc_freq
        self.doc_lengths[doc_id] = len(tokens)
        
        # í‰ê·  ë¬¸ì„œ ê¸¸ì´ ì¬ê³„ì‚°
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0
        
        # ì—­ìƒ‰ì¸ ì—…ë°ì´íŠ¸
        for token in set(tokens):
            if token not in self.keyword_index:
                self.keyword_index[token] = []
            if doc_id not in self.keyword_index[token]:
                self.keyword_index[token].append(doc_id)
    
    def _save_entry_data(self, entry_id: int, data: Dict[str, Any]):
        """ì—”íŠ¸ë¦¬ ë°ì´í„°ë¥¼ ë³„ë„ íŒŒì¼ì— ì €ì¥"""
        data_dir = self.db_path / "entry_data"
        data_dir.mkdir(exist_ok=True)
        
        data_file = data_dir / f"entry_{entry_id}.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
    
    def _load_entry_data(self, entry_id: int) -> Dict[str, Any]:
        """ì—”íŠ¸ë¦¬ ë°ì´í„°ë¥¼ ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        data_file = self.db_path / "entry_data" / f"entry_{entry_id}.json"
        
        if not data_file.exists():
            return {}
        
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    
    def get_entry_with_data(self, entry_id: int) -> Dict[str, Any]:
        """ì—”íŠ¸ë¦¬ IDë¡œ ì™„ì „í•œ ë°ì´í„° ì¡°íšŒ"""
        if entry_id >= len(self.metadata):
            return {}
        
        metadata = self.metadata[entry_id].copy()
        metadata["data"] = self._load_entry_data(entry_id)
        metadata["text"] = self.data_entries[entry_id]
        
        return metadata
    
    def _personal_info_to_text(self, personal_info: Dict[str, Any]) -> str:
        """ê°œì¸ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"ì´ë¦„: {personal_info.get('name', '')}")
        parts.append(f"ì´ë©”ì¼: {personal_info.get('email', '')}")
        parts.append(f"ì „í™”ë²ˆí˜¸: {personal_info.get('phone', '')}")
        parts.append(f"ê±°ì£¼ì§€: {personal_info.get('location', '')}")
        return " ".join(parts)
    
    def _education_to_text(self, education: Dict[str, Any]) -> str:
        """í•™ë ¥ ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"í•™êµ: {education.get('university', '')}")
        parts.append(f"ì „ê³µ: {education.get('major', '')}")
        parts.append(f"í•™ìœ„: {education.get('degree', '')}")
        parts.append(f"ì¡¸ì—…ë…„ë„: {education.get('graduation_year', '')}")
        parts.append(f"í•™ì : {education.get('gpa', '')}")
        parts.extend(education.get('relevant_courses', []))
        parts.extend(education.get('honors', []))
        return " ".join(parts)
    
    def _work_experience_to_text(self, experience: Dict[str, Any]) -> str:
        """ê²½ë ¥ ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"íšŒì‚¬: {experience.get('company', '')}")
        parts.append(f"ì§ì±…: {experience.get('position', '')}")
        parts.append(f"ë¶€ì„œ: {experience.get('department', '')}")
        parts.append(f"ê¸°ê°„: {experience.get('duration', {}).get('start', '')} ~ {experience.get('duration', {}).get('end', '')}")
        parts.extend(experience.get('responsibilities', []))
        
        # ì„±ê³¼ ì •ë³´
        for achievement in experience.get('achievements', []):
            parts.append(f"ì„±ê³¼: {achievement.get('description', '')}")
            parts.append(f"ì§€í‘œ: {achievement.get('metrics', '')}")
            parts.append(f"ì„íŒ©íŠ¸: {achievement.get('impact', '')}")
        
        parts.extend(experience.get('technologies', []))
        parts.append(f"íŒ€ê·œëª¨: {experience.get('team_size', '')}")
        parts.extend(experience.get('key_projects', []))
        
        return " ".join(parts)
    
    def _project_to_text(self, project: Dict[str, Any]) -> str:
        """í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"í”„ë¡œì íŠ¸ëª…: {project.get('name', '')}")
        parts.append(f"ìœ í˜•: {project.get('type', '')}")
        parts.append(f"ê¸°ê°„: {project.get('duration', {}).get('start', '')} ~ {project.get('duration', {}).get('end', '')}")
        parts.append(f"ì„¤ëª…: {project.get('description', '')}")
        parts.append(f"ì—­í• : {project.get('role', '')}")
        parts.extend(project.get('technologies', []))
        parts.append(f"ì„±ê³¼: {project.get('achievements', '')}")
        parts.append(f"íŒ€ê·œëª¨: {project.get('team_size', '')}")
        
        return " ".join(parts)
    
    def _skills_to_text(self, skills: Dict[str, Any]) -> str:
        """ê¸°ìˆ  ìŠ¤íƒì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        for category, skill_list in skills.items():
            if isinstance(skill_list, list):
                parts.extend(skill_list)
        return " ".join(parts)
    
    def _certification_to_text(self, certification: Dict[str, Any]) -> str:
        """ìê²©ì¦ ì •ë³´ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"ìê²©ì¦ëª…: {certification.get('name', '')}")
        parts.append(f"ë°œê¸‰ê¸°ê´€: {certification.get('issuer', '')}")
        parts.append(f"ì·¨ë“ì¼: {certification.get('date', '')}")
        parts.append(f"ì ìˆ˜: {certification.get('score', '')}")
        return " ".join(parts)
    
    def _award_to_text(self, award: Dict[str, Any]) -> str:
        """ìˆ˜ìƒë‚´ì—­ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"ìˆ˜ìƒëª…: {award.get('name', '')}")
        parts.append(f"ìˆ˜ì—¬ê¸°ê´€: {award.get('issuer', '')}")
        parts.append(f"ìˆ˜ìƒì¼: {award.get('date', '')}")
        parts.append(f"ë‚´ìš©: {award.get('description', '')}")
        return " ".join(parts)
    
    def _career_goals_to_text(self, goals: Dict[str, Any]) -> str:
        """ì»¤ë¦¬ì–´ ëª©í‘œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        parts.append(f"ë‹¨ê¸°ëª©í‘œ: {goals.get('short_term', '')}")
        parts.append(f"ì¥ê¸°ëª©í‘œ: {goals.get('long_term', '')}")
        parts.extend(goals.get('target_companies', []))
        parts.extend(goals.get('preferred_roles', []))
        return " ".join(parts)
    
    def _interests_to_text(self, interests: List[str]) -> str:
        """ê´€ì‹¬ì‚¬ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        return " ".join(interests)
    
    def save_db(self):
        """ë²¡í„°DB ì €ì¥"""
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        index_path = self.db_path / "unified_faiss_index.bin"
        faiss.write_index(self.index, str(index_path))
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = self.db_path / "unified_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump({
                "data_entries": self.data_entries,
                "metadata": self.metadata
            }, f, ensure_ascii=False, indent=2)
        
        print(f"í†µí•© ë²¡í„°DB ì €ì¥ ì™„ë£Œ: {self.db_path}")
    
    def _load_existing_db(self):
        """ê¸°ì¡´ í†µí•© ë²¡í„°DB ë¡œë“œ"""
        index_path = self.db_path / "unified_faiss_index.bin"
        metadata_path = self.db_path / "unified_metadata.json"
        
        if index_path.exists() and metadata_path.exists():
            try:
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(str(index_path))
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.data_entries = data["data_entries"]
                    self.metadata = data["metadata"]
                
                print(f"ê¸°ì¡´ í†µí•© ë²¡í„°DB ë¡œë“œ ì™„ë£Œ: {len(self.data_entries)}ê°œ ì—”íŠ¸ë¦¬")
            except Exception as e:
                print(f"DB ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ì‹œì‘: {e}")
    
    def get_db_stats(self) -> Dict[str, Any]:
        """ë²¡í„°DB í†µê³„ ë°˜í™˜"""
        type_counts = {}
        profile_counts = {}
        
        for entry in self.metadata:
            entry_type = entry.get("type", "unknown")
            profile_name = entry.get("profile_name", "unknown")
            
            type_counts[entry_type] = type_counts.get(entry_type, 0) + 1
            profile_counts[profile_name] = profile_counts.get(profile_name, 0) + 1
        
        return {
            "total_entries": len(self.data_entries),
            "type_counts": type_counts,
            "profile_counts": profile_counts,
            "index_size": self.index.ntotal
        } 

    def _remove_profile_entries(self, profile_name: str):
        """íŠ¹ì • í”„ë¡œí•„ì˜ ëª¨ë“  ì—”íŠ¸ë¦¬ ì œê±°"""
        if not self.metadata:
            return
        
        # ì œê±°í•  ì¸ë±ìŠ¤ ì°¾ê¸°
        indices_to_remove = []
        for i, metadata in enumerate(self.metadata):
            if metadata.get("profile_name") == profile_name:
                indices_to_remove.append(i)
        
        if not indices_to_remove:
            return
        
        print(f"ğŸ—‘ï¸  ê¸°ì¡´ í”„ë¡œí•„ '{profile_name}' ì—”íŠ¸ë¦¬ {len(indices_to_remove)}ê°œ ì œê±° ì¤‘...")
        
        # ì—­ìˆœìœ¼ë¡œ ì œê±° (ì¸ë±ìŠ¤ ë³€ê²½ ë°©ì§€)
        for idx in reversed(indices_to_remove):
            del self.data_entries[idx]
            del self.metadata[idx]
        
        # FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ì •í™•í•¨)
        self._rebuild_faiss_index()
    
    def _rebuild_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        if not self.data_entries:
            self.index = faiss.IndexFlatIP(self.dimension)
            return
        
        # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
        self.index = faiss.IndexFlatIP(self.dimension)
        
        # ëª¨ë“  ì—”íŠ¸ë¦¬ ë‹¤ì‹œ ì„ë² ë”©í•˜ì—¬ ì¶”ê°€
        embeddings = self.encoder.encode(self.data_entries)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings) 

    def _semantic_search(self, query: str, profile_name: str, data_types: List[str], 
                        top_k: int, min_score: float) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ (ë°ì´í„°/AI íŠ¹í™” ê°€ì¤‘ì¹˜ ì ìš©)"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìºì‹±
        cache_key = f"semantic_{hash(query)}"
        if cache_key not in self.query_cache:
            query_embedding = self.encoder.encode([query])
            faiss.normalize_L2(query_embedding)
            self.query_cache[cache_key] = query_embedding
        else:
            query_embedding = self.query_cache[cache_key]
        
        # ê²€ìƒ‰ (ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§)
        search_k = min(top_k * 3, self.index.ntotal)
        scores, indices = self.index.search(query_embedding, search_k)
        
        # ë°ì´í„°/AI íŠ¹í™” íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        type_weights = {
            # ë°ì´í„°/AI í•µì‹¬ ê²½í—˜ (ìµœê³  ê°€ì¤‘ì¹˜)
            "work_experience": 1.5,    # ì‹¤ë¬´ ê²½í—˜ì´ ê°€ì¥ ì¤‘ìš”
            "project": 1.4,            # í”„ë¡œì íŠ¸ ê²½í—˜ë„ ë§¤ìš° ì¤‘ìš”
            
            # ê¸°ìˆ ì  ì—­ëŸ‰
            "skills": 1.3,             # ê¸°ìˆ  ìŠ¤íƒì´ ë§¤ìš° ì¤‘ìš”
            "certifications": 1.2,     # ë°ì´í„°/AI ìê²©ì¦ ì¤‘ìš”
            
            # í•™ìŠµ/ì—°êµ¬ ë°°ê²½
            "education": 1.1,          # í•™ë¬¸ì  ë°°ê²½ ì¤‘ìš” (í†µê³„, ìˆ˜í•™, CS)
            "research": 1.3,           # ì—°êµ¬ ê²½í—˜ (ìƒˆë¡œìš´ íƒ€ì…)
            "publications": 1.2,       # ë…¼ë¬¸/ì¶œê°„ë¬¼ (ìƒˆë¡œìš´ íƒ€ì…)
            
            # ë¶€ê°€ì  ìš”ì†Œ
            "career_goals": 1.0,       # ì»¤ë¦¬ì–´ ëª©í‘œ
            "personal_info": 0.9,      # ê°œì¸ì •ë³´
            "award": 1.1,              # ìˆ˜ìƒ ê²½ë ¥ (ë°ì´í„° ê²½ì§„ëŒ€íšŒ ë“±)
            "interests": 0.8,          # ê´€ì‹¬ì‚¬
            
            # ë°ì´í„°/AI íŠ¹í™” ìƒˆë¡œìš´ íƒ€ì…ë“¤
            "kaggle_competitions": 1.3,  # ìºê¸€ ê²½ì§„ëŒ€íšŒ
            "data_projects": 1.4,        # ë°ì´í„° í”„ë¡œì íŠ¸
            "ml_models": 1.3,            # ML ëª¨ë¸ ê°œë°œ
            "analytics_reports": 1.1,    # ë¶„ì„ ë¦¬í¬íŠ¸
            "data_pipelines": 1.2,       # ë°ì´í„° íŒŒì´í”„ë¼ì¸
            "dashboards": 1.0,           # ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
            "ab_tests": 1.1,             # A/B í…ŒìŠ¤íŠ¸ ê²½í—˜
            "feature_engineering": 1.2,  # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
            "model_deployment": 1.3,     # ëª¨ë¸ ë°°í¬
            "data_visualization": 1.0    # ë°ì´í„° ì‹œê°í™”
        }
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx >= 0 and score >= min_score:
                metadata = self.metadata[idx]
                
                # í”„ë¡œí•„ í•„í„°ë§
                if profile_name and metadata.get("profile_name") != profile_name:
                    continue
                
                # ë°ì´í„° ìœ í˜• í•„í„°ë§
                if data_types and metadata.get("type") not in data_types:
                    continue
                
                # íƒ€ì…ë³„ ê°€ì¤‘ì¹˜ ì ìš©
                entry_type = metadata.get("type", "unknown")
                weighted_score = float(score) * type_weights.get(entry_type, 1.0)
                
                # ë°ì´í„°/AI í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ ì ìˆ˜
                text = self.data_entries[idx].lower()
                data_ai_keywords = [
                    "python", "pandas", "numpy", "scikit-learn", "tensorflow", "pytorch",
                    "machine learning", "deep learning", "data science", "analytics",
                    "sql", "spark", "hadoop", "kafka", "airflow", "tableau", "powerbi",
                    "statistics", "regression", "classification", "clustering", "nlp",
                    "computer vision", "recommendation", "time series", "a/b test"
                ]
                
                keyword_bonus = 0.0
                for keyword in data_ai_keywords:
                    if keyword in text:
                        keyword_bonus += 0.05  # ê° í‚¤ì›Œë“œë§ˆë‹¤ 5% ë³´ë„ˆìŠ¤
                
                final_score = weighted_score * (1.0 + min(keyword_bonus, 0.3))  # ìµœëŒ€ 30% ë³´ë„ˆìŠ¤
                
                results.append({
                    "metadata": metadata,
                    "score": final_score,
                    "original_score": float(score),
                    "type_weight": type_weights.get(entry_type, 1.0),
                    "keyword_bonus": keyword_bonus,
                    "text": self.data_entries[idx],
                    "search_method": "semantic_similarity_data_ai"
                })
        
        # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì ìˆ˜ë¡œ ì •ë ¬
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:top_k]
    
    def _keyword_search(self, query: str, profile_name: str, data_types: List[str], 
                       top_k: int, min_score: float) -> List[Dict[str, Any]]:
        """BM25 ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        if not self.keyword_index:
            self._build_keyword_index()
        
        # ì¿¼ë¦¬ í† í°í™”
        query_tokens = self._tokenize(query)
        
        # BM25 ì ìˆ˜ ê³„ì‚°
        bm25_scores = self._calculate_bm25_scores(query_tokens)
        
        # ì ìˆ˜ê°€ ìˆëŠ” ë¬¸ì„œë“¤ë§Œ í•„í„°ë§
        candidate_docs = [(doc_id, score) for doc_id, score in enumerate(bm25_scores) if score > min_score]
        candidate_docs.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for doc_id, score in candidate_docs[:top_k * 2]:  # ì—¬ìœ ìˆê²Œ ê°€ì ¸ì˜¤ê¸°
            if doc_id >= len(self.metadata):
                continue
                
            metadata = self.metadata[doc_id]
            
            # í”„ë¡œí•„ í•„í„°ë§
            if profile_name and metadata.get("profile_name") != profile_name:
                continue
            
            # ë°ì´í„° ìœ í˜• í•„í„°ë§
            if data_types and metadata.get("type") not in data_types:
                continue
            
            results.append({
                "metadata": metadata,
                "score": score,
                "text": self.data_entries[doc_id],
                "search_method": "keyword_bm25"
            })
        
        return results[:top_k]
    
    def _build_keyword_index(self):
        """í‚¤ì›Œë“œ ì¸ë±ìŠ¤ êµ¬ì¶• (BM25ìš©)"""
        self.keyword_index = {}
        self.doc_frequencies = []
        self.doc_lengths = []
        
        for doc_id, text in enumerate(self.data_entries):
            tokens = self._tokenize(text)
            doc_freq = Counter(tokens)
            self.doc_frequencies.append(doc_freq)
            self.doc_lengths.append(len(tokens))
            
            # ì—­ìƒ‰ì¸ êµ¬ì¶•
            for token in set(tokens):
                if token not in self.keyword_index:
                    self.keyword_index[token] = []
                self.keyword_index[token].append(doc_id)
        
        # í‰ê·  ë¬¸ì„œ ê¸¸ì´ ê³„ì‚°
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0
    
    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í°í™” (í•œê¸€/ì˜ë¬¸ ì§€ì›)"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text.lower())
        tokens = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ìˆ', 'í•˜', 'ë˜', 'ë ', 'í•œ', 'ì¼', 'ë•Œ', 'ì¤‘', 'ë°', 'ë“±', 
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'
        }
        
        return [token for token in tokens if len(token) > 1 and token not in stop_words]
    
    def _calculate_bm25_scores(self, query_tokens: List[str]) -> List[float]:
        """BM25 ì ìˆ˜ ê³„ì‚°"""
        k1, b = 1.5, 0.75  # BM25 íŒŒë¼ë¯¸í„°
        N = len(self.data_entries)  # ì „ì²´ ë¬¸ì„œ ìˆ˜
        
        scores = [0.0] * N
        
        for token in query_tokens:
            if token not in self.keyword_index:
                continue
            
            # í•´ë‹¹ í† í°ì„ í¬í•¨í•œ ë¬¸ì„œë“¤
            docs_with_token = self.keyword_index[token]
            df = len(docs_with_token)  # ë¬¸ì„œ ë¹ˆë„
            
            # IDF ê³„ì‚°
            idf = math.log((N - df + 0.5) / (df + 0.5))
            
            for doc_id in docs_with_token:
                # TF ê³„ì‚°
                tf = self.doc_frequencies[doc_id].get(token, 0)
                doc_len = self.doc_lengths[doc_id]
                
                # BM25 ì ìˆ˜ ê³„ì‚°
                score = idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * doc_len / self.avg_doc_length))
                scores[doc_id] += score
        
        return scores 